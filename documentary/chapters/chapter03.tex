Since the 1970s \parencite{article14}, the decade in which the microprocessor era started, the overall performance of a processor has increased \parencite{inproceedings4}. This goal was achieved by several points, including "sophisticated process technology, innovative architecture or micro-architecture" \parencite[see][Chapter 1, p2]{inproceedings4}. In fact, increasing the clock speed of a single core processor, like Moore's Law predicted \parencite{article14}, was usually reached by increasing the number of transistors on the chip \parencite{article14}. However, this go along side with the increase in complexity \parencite[see][Pollack’s rule]{article14}, which mean, that doubling the logic of a processor result in a performance boost of only 40\% \parencite[see][Chapter 2]{article14}.

Another huge problem chip manufacturers have to deal with is leakage power \parencite[see][Chapter 2, p3]{inproceedings4}, because the "transistor leakage current increases as the chip size shrinks" \parencite[see][p2]{article14} [see Chart \ref{fig:leackagePer}]. An increase of leakage current of the transistors also result in a increase of the die's temperature \parencite{inproceedings4} along side the total power consumption as well.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.25]{leackage-power_vs_process-technology.png}
	\caption{
		Leakage Power (\% of total) vs. process technology \parencite{inproceedings4}
	}
	\label{fig:leackagePer}
\end{figure}

Furthermore, a increase of the processor clock frequency to speed up the performance is only available to a suffisticating limit of 4GHz \parencite{article14}. After this frequency threshold, also known as reaching the power wall, the "power dissipation" \parencite[see][p2]{article14} increases again.

Facing these types of problems such as "chip fabrication costs, fault tolerance, power efficiency, heat dissipation" \parencite[see][p3]{article14} along side with increasing processor performance, the only possible solution chip manufacturers and companies could offer was parallelism. 

\newpage

\section{Basic Concept}

Parallelism for programming is not something new. But due to the fact that real thread level parallelism [see Chapter \ref{subchap:threadLevelParallelism}] was only available after dual or multi-core processors were invented in 2005 \parencite{internet6}, the topic itself and efficient software implementations are still treated in scientific work like \parencite{book3} \parencite{article2}.

In general, parallelism for programming means to split up a task or a computation into several sub tasks or results, to decrease the execution time. Depending on the problem itself, this separated tasks can be independent or connected. If we want to talk about the general concept of parallelism, we have to take a closer look to some mathematical laws, which try to describe the availability to parallel task execution and their limits.\\\\
\
The first one is called Amdahl's Law \parencite{inbook1}. During the period of publication of Amdahl's paper \parencite{book6}, critics claimed "that the organization of a single computer has reached its limits and that
truly significant advances can be made only by interconnection of a multiplicity of computers” \parencite[see][p80]{inbook1}. Ofcourse this can be transfered on single and multi-core processors or even on multi threading, but in fact, like Amdahl claimed too, addressing hardware \parencite{inbook1}, and nowadays switching context time was not considered in this case.

Amdahl's Law wants "to provide an upper limit on speedup" \parencite[see][p81]{inbook1} in general to point out, that there is a overhead \parencite{inbook1}, which can not pe implemented in parallel, but at the same time, "apart from the sequential fraction, the remaining computations are perfectly parallelizable" \parencite[see][p81]{inbook1}. 

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.95]{amdahls-law.png}
	\caption{
		The limited speed-up of a program, which can be parallelized, depending on the number of parallel executions \parencite{image1}.
	}
	\label{fig:admLaw}
\end{figure}

\newpage

\begin{itemize}
\item Valiant noted in 1990, “no substantial impediments to general-purpose parallel
computation” exist \parencite[see][p85]{inbook1}, though there are limits, as shown \parencite[seein Sec. 10][p85]{inbook1}.
\item “the fraction of the computational load... associated with data management housekeeping ... accounts for 40\% of the executed instructions”, "this overhead appears to be sequential so that it is unlikely to be amenable to parallel processing techniques” \parencite[see][p80]{inbook1}; amount of overhead (data management and addressing based on hardware restricitons), which can not be parallelized and reduces the factor of speed increase after parallelization. According to Amdahl, this factor can be reduced, but not with "parallel processing techniques", probably with more precisely and efficent hardware.
\item a resisting "upper limit on speedup [exists] and therefore, apart from the sequential fraction (the so called non parallelizabled overhead), the remaining computations are perfectly parallelizable" \parencite[see][p81]{inbook1}
\item the law in general: Let t\textsubscript{1} be the time taken by one processor solving a computational problem and t\textsubscript{p} be the time taken by p processors solving the same problem. Finally let us denote the supposed inherently sequential fraction of instructions by f. Then, according to Amdahl, t\textsubscript{p} = t\textsubscript{1} (f+(1-f)/p) and the speedup obtainable by p processors can be expressed as:

	
\[ \frac{t_1}{t_p} = \frac{1}{f + (1 - f) / p)} \]
\end{itemize}

\underline{Gustafson’s Law's} \parencite{inbook1}:
\begin{itemize}
	\item ...\parencite[see][p81]{inbook1}
	\item ...\parencite[see][p87]{inbook1}\\
\end{itemize}

\underline{Concurrency in general}:
\begin{itemize}
	\item ...\parencite[see][p3]{internet1}
	\item ...\parencite[see][p3]{internet2}
	\item ...
\end{itemize}

\newpage

\subsection{Principles of Parallel Computing}

\textbf{Emphasises design}:
Amdahl's law highlights the pitfalls of looking for
sticking-plaster speed-ups in serial programs –
design for concurrency \parencite[see][p4]{article6}
\\\\\textbf{Aim of concurrency and their effects} on program structure or implementation \parencite[see][p5]{article6}:
\begin{itemize}
  \item \underline{Flexibility}: Environments will be more heterogeneous.
  \item \underline{Efficiency}:	parallel for a speed-up purposes, more pitfalls (memory latency, thread overheads etc.)
  \item \underline{Simplicity}:	Parallel codes will be more complicated. All the more reason to strive for maintainable, understandable programs.\\
\end{itemize}
...\parencite[see][p11 ff.]{article6}

\newpage

\section{Definition of parallel mathematical computations}

\underline{Mathematical examples} \parencite{inbook1}:
\begin{itemize}
	\item ...\parencite[see][p8]{internet1}
	\item ...\parencite[see][p4]{internet2}
	\item ...\parencite[see][p398]{article7}
\end{itemize}

\newpage

\section{Parallel Computer Architecture}

...\parencite[see][p9]{book1} \\
...examples of parallelism \parencite[see][p11]{book1} \\
...links 1) \\
...\parencite[see][p9 ff.]{book5}


\subsection{Flynn's Taxonomy of Parallel Architectures}

...\parencite[see][p5]{internet1}\\
...\parencite[see][p13]{book1}\\
...\parencite[see][p4]{book5}\\
...\parencite[see][p2]{book6}\\
...\parencite[see][p15-p26]{internet2}

\subsection{Thread Level Parallelism}\label{subchap:threadLevelParallelism}

...\parencite[see][p24]{book1}\\
...\parencite[see][p14]{article6}

\section{Parallel Programming Models}

\textbf{Steps to evaluate a proper parallel design} \parencite[see][p6]{article6}:
\begin{enumerate}
	\item Finding Concurrency
	\item Algorithm Structure
	\item Supporting Structures
	\item Implementation Mechanisms
\end{enumerate}

\newpage

\subsection{Classification of Parallel Programming Models}

\subsubsection{Process Interaction}

...\parencite[see][p4]{internet1}

\subsubsection{Problem decomposition}

...\parencite[see][p105 ff.]{book1}